<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/css/app.css">

    <style>
      .observablehq {
          opacity: 1;
          transition: opacity 250ms ease-in-out;
      }

      .browser-firefox .observablehq {
          transition: opacity 250ms ease-in-out 250ms;
      }

      .observablehq--running {
          opacity: 0;
      }

      .os-ios #gradient {
          filter: saturate(150%);
          -webkit-filter: saturate(150%);
      }
  </style>

    <script src="/js/main.js" defer></script>
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700|Lora" rel="stylesheet">
    <link rel="icon" type="image/png" href="/favicon.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16" />
    <meta name="generator" content="AI4CC" />
    <title>AI4CC | Our Work | Object Detection with RCNN's</title>
  </head>
  <body>


<div class="app-root">

  <!-- START OF: Header =====-->
  <div class="header-container-nav">

    <header class="header-1">     <!--style="background-image: url(img/photos/robot-01.jpg)"> -->

      <nav class="nav-1">
          
          <div class="logos">
            <div class="nav-logo">
              <a class="nav-link" href="/">
                <img class="nav-logo-img"   src="/img/other/logo-sk.svg" alt="Logo"/>
              </a>
            </div>
            <div class="nav-logo">
              <a class="nav-link" href="/">
                <span class="nav-logo-text color-white larger-text">AI4CC</span>
              </a>
            </div>
          </div>
          <div class="nav-all-links">
            <ul class="nav-links">
              <li><a class="nav-item color-white" href="#"> About</a></li>
              <li><a class="nav-item color-white" href="/people/"> Our Team</a></li>
              <li><a class="nav-item color-white" id="last-nav-item" href="/our-work/"> Our Work</a></li>
            </ul>
          </div>

          <div id="nav-icon" >
            <span class="bg-white"></span>
            <span class="bg-white"></span>
            <span class="bg-white"></span>
            <span class="bg-white"></span>
          </div>
      </nav>

    </header>

    <div class="position-absolute w-100 h-100 overflow-hidden bg-cool-gray-1 bg-cover trbl-0" style="z-index:-1;pointer-events:none">
      <div id="gradient" class="position-absolute d-flex justify-content-center align-items-center overflow-hidden trbl-0 observablehq--running" style="z-index:-1;pointer-events:none"></div>
    </div>

  </div>

  <div class="all-content">
  <main>

    <div class="header-window">
      <div class="container">
        <hr class="hr-strong">
        <section class="header-pm header-container">
          <div class="header-headline font-normal mb-1"><h1>AI4CC Applications</h1></div>
        </section>
          
      </div>
    </div>  

    <div class="publication-window">
        <div class="container">
            <hr class="hr-strong">
            <section class="publication-container publication-pm">
                <div class="post">
                    <h2 class="title">
                        <a href="./" title="Object Detection with RCNN's">Object Detection with RCNN's</a>
                    </h2>
                    <div class="post-header">
                        <span class="publish-date">
                            Wednesday, June 24, 2020
                        </span>
                    </div>
                    <div class="post-body">
                        <div class="post-author">
                            Posted by Martin Goofrey, Visiting Faculty Researcher and Tom Clooney, Research Scientist, AI4CC
                        </div>
                        <div class="post-content">
                            <p>However, more recent approaches, like T5, have also shown that neural models, trained on large amounts of web-text, can also answer questions directly, without retrieving documents or facts from a knowledge graph. This has led to significant debate about how knowledge should be stored for use by our question answering systems — in human readable text and structured formats, or in the learned parameters of a neural network.</p>
                            <p>However, more recent approaches, like T5, have also shown that neural models, trained on large amounts of web-text, can also answer questions directly, without retrieving documents or facts from a knowledge graph. This has led to significant debate about how knowledge should be stored for use by our question answering systems — in human readable text and structured formats, or in the learned parameters of a neural network.</p>
                            <strong>Purpose</strong>
                            <p>Repeating processes ranging from natural cycles, such as phases of the moon or heartbeats and breathing, to artificial repetitive processes, like those found on manufacturing lines or in traffic patterns, are commonplace in our daily lives. Beyond just their prevalence, repeating processes are of interest to researchers for the variety of insights one can tease out of them. It may be that there is an underlying cause behind something that happens multiple times, or there may be gradual changes in a scene that may be useful for understanding. Sometimes, repeating processes provide us with unambiguous “action units”, semantically meaningful segments that make up an action. For example, if a person is chopping an onion, the action unit is the manipulation action that is repeated to produce additional slices. These units may be indicative of more complex activity and may allow us to analyze more such actions automatically at a finer time-scale without having a person annotate these units. For the above reasons, perceptual systems that aim to observe and understand our world for an extended period of time will benefit from a system that understands general repetitions.</p>
                            <img border="0" data-original-height="228" data-original-width="772" src="https://1.bp.blogspot.com/-RzCBrCZykLE/XvDpz5viqyI/AAAAAAAAGIU/Ax43DDRjtWkeAZsepspakwUEGkshGsqQwCLcBGAsYHQ/s640/image9.png">
                            
                            <strong>Result</strong>
                            <p>Artificial repetitive processes, like those found on manufacturing lines or in traffic patterns, are commonplace in our daily lives. Beyond just their prevalence, repeating processes are of interest to researchers for the variety of insights one can tease out of them. It may be that there is an underlying cause behind something that happens multiple times, or there may be gradual changes in a scene that may be useful for understanding. Sometimes, repeating processes provide us with unambiguous “action units”, semantically meaningful segments that make up an action. For example, if a person is chopping an onion, the action unit is the manipulation action that is repeated to produce additional slices. These units may be indicative of more complex activity and may allow us to analyze more such actions automatically at a finer time-scale without having a person annotate these units. For the above reasons, perceptual systems that aim to observe and understand our world for an extended period of time will benefit from a system that understands general repetitions.</p>
                            
                        </div>
                    </div>

                </div>
            </section>
        </div>
    </div>
    



  </main>

  </div>

</div>

<script type="module">
  import {Runtime, Inspector, Library} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@4/dist/runtime.js";
  import notebook from "/js/home.js";
  const renders = {
  "gradient": "#gradient",
  };
  new Runtime(Object.assign(new Library)).module(notebook, name => {
  const selector = renders[name];
  if (selector) { // key exists
      return new Inspector(document.querySelector(selector));
  } else {
      return true;
  }
  });
</script>

  </body>
</html>