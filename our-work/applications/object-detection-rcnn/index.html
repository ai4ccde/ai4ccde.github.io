<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/css/app.css">
    <script src="/js/main.js" defer></script>
    <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@400;700&display=swap" rel="stylesheet">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <meta name="generator" content="AI4CC" />
    <title>AI4CC | Our Work | Object Detection with RCNN's</title>
  </head>
  <body>


<div class="app-root">

  <!-- START OF: Header =====-->
  <div class="header-container-nav">

    <header class="header-1">     <!--style="background-image: url(img/photos/robot-01.jpg)"> -->

      <nav class="nav-1">
          
          <div class="logos">
            <div class="nav-logo">
              <a class="nav-link" href="/">
                <img class="nav-logo-img"   src="/img/other/logo.png" alt="Logo"/>
              </a>
            </div>
            <div class="nav-logo">
              <a class="nav-link" href="/">
                <span class="nav-logo-text color-white larger-text">AI4CC</span>
              </a>
            </div>
          </div>
          <div class="nav-all-links">
            <ul class="nav-links">
              <li><a class="nav-item color-white" href="#"> About</a></li>
              <li><a class="nav-item color-white" href="/people/"> Our Team</a></li>
              <li><a class="nav-item color-white" id="last-nav-item" href="/our-work/"> Our Work</a></li>
            </ul>
          </div>

          <div id="nav-icon" >
            <span class="bg-white"></span>
            <span class="bg-white"></span>
            <span class="bg-white"></span>
            <span class="bg-white"></span>
          </div>
      </nav>

    </header>

  </div>

  <div class="all-content">
  <main>

    <div class="header-window">
      <div class="container">
        <hr class="hr-strong">
        <section class="header-pm header-container">
          <div class="header-headline font-normal mb-1"><h1>AI4CC Applications</h1></div>
        </section>
          
      </div>
    </div>  

    <div class="publication-window">
        <div class="container">
            <hr class="hr-strong">
            <section class="publication-container publication-pm">
                <div class="post">
                    <h2 class="title">
                        <a href="./" title="Object Detection with RCNN's">Object Detection with RCNN's</a>
                    </h2>
                    <div class="post-header">
                        <span class="publish-date">
                            Wednesday, June 24, 2020
                        </span>
                    </div>
                    <div class="post-body">
                        <div class="post-author">
                            Posted by <a href="/people/scientific-staff/martin-goodfellow/">Martin Goofrey</a>, Visiting Faculty Researcher and Tom Clooney, Research Scientist, AI4CC
                        </div>
                        <div class="post-content">
                            <aside class="aside color-fg-50 small-text">
                              <div class="mb-0.125">Content</div>
                              <ol class="list-indented">
                                <li><a class="no-underline" href="#purpose">Purpose</a></li>
                                <li><a class="no-underline" href="#result">Result</a></li>
                                <li><a class="no-underline" href="#purpose">Purpose</a></li>
                              </ol>
                            </aside>
                            <hr class="my-1 hr-strong d-m-none">
                            <!-- <aside class="footnote-aside">This new object dete</aside> -->
                            <p>However, more recent approaches, like T5, have also shown that neural models, trained on large amounts of web-text, can also answer questions directly, without retrieving documents or facts from a knowledge graph. This has led to significant debate about how knowledge should be stored for use by our question answering systems — in human readable text and structured formats, or in the learned parameters of a neural network.</p>
                            
                            <p>However, more recent approaches, like T5, have also shown that neural models, trained on large amounts of web-text, can also answer questions directly, without retrieving documents or facts from a knowledge graph. <sup class="footnote-ref"><a href="#fn-1" id="fnref-1">[1]</a></sup>   <span class="sidenote">This new object detection architecture leverages contextual clues across time for each camera deployment in a network, improving recognition of objects in novel camera. </span>  This has led to significant debate about how knowledge should be stored for use by our question answering systems — in human readable text and structured formats, or in the learned parameters of a neural network.</p>
                            <strong id="purpose">Purpose</strong>                           <p>Repeating processes ranging from natural cycles, such as phases of the moon or heartbeats and breathing, to artificial repetitive processes, like those found on manufacturing lines or in traffic patterns, are commonplace in our daily lives. Beyond just their prevalence, repeating processes are of interest to researchers for the variety of insights one can tease out of them. It may be that there is an underlying cause behind something that happens multiple times, or there may be gradual changes in a scene that may be useful for understanding. Sometimes, repeating processes provide us with unambiguous “action units”, semantically meaningful segments that make up an action. For example, if a person is chopping an onion, the action unit is the manipulation action that is repeated to produce additional slices. These units may be indicative of more complex activity and may allow us to analyze more such actions automatically at a finer time-scale without having a person annotate these units. For the above reasons, perceptual systems that aim to observe and understand our world for an extended period of time will benefit from a system that understands general repetitions.</p>
                            <div class="img-holder">
                              <img border="0" src="https://1.bp.blogspot.com/-RzCBrCZykLE/XvDpz5viqyI/AAAAAAAAGIU/Ax43DDRjtWkeAZsepspakwUEGkshGsqQwCLcBGAsYHQ/s640/image9.png">
                            </div>
                            <strong id="result">Result</strong>
                            <p>Artificial repetitive processes, like those found on manufacturing lines or in traffic patterns, are commonplace in our daily lives. Beyond just their prevalence, repeating processes are of interest to researchers for the variety of insights one can tease out of them. It may be that there is an underlying cause behind something that happens multiple times, or there may be gradual changes in a scene that may be useful for understanding. Sometimes, repeating processes provide us with unambiguous “action units”, semantically meaningful segments that make up an action. For example, if a person is chopping an onion, the action unit is the manipulation action that is repeated to produce additional slices. These units may be indicative of more complex activity and may allow us to analyze more such actions automatically at a finer time-scale without having a person annotate these units. For the above reasons, perceptual systems that aim to observe and understand our world for an extended period of time will benefit from a system that understands general repetitions.</p>
                            <hr class="hr-strong my-1">
                            <div class="footnotes">
                              Footnotes:
                              <ol class="footnotes-list">
                                <li id="fn-1">
                                  This new object detection architecture leverages contextual clues across time for each camera deployment in a network, improving recognition of objects in novel camera. <a href="#fnref-1" class="footnote-backref">↩︎</a>
                                </li>
                              </ol>
                            </div>

                        </div>
                    </div>

                </div>
            </section>
        </div>
    </div>
    



  </main>

  </div>

</div>


  </body>
</html>